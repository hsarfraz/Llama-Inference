# Llama2-Inference
Setting up Llama LLM inference On-Premise Environment

Large language models (LLMs) are a powerful tool with the potential to revolutionize a wide range of industries. However, deploying and managing LLMs can be a complex and challenging task. This repo provides implemented details to perform LLMs in an on-premise environment. 

##Option No 1: LLM Inference Server with Custom REST APIs

##Option No 2: LLM Inference Server â€“ Using Huggingface TRL

